{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as kb\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "# Tensorflow untils packages.\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "import pathlib # for processing a path e.g., c:\\documents\\files\\test_ds.csv\n",
    "import matplotlib.pyplot as plt # for plotting data and creating different charts.\n",
    "import numpy as np # for math and arrays\n",
    "import pandas as pd # data from for the data.\n",
    "import seaborn as sns # for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>62</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.127</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>82</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>38.2</td>\n",
       "      <td>0.233</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>68</td>\n",
       "      <td>42</td>\n",
       "      <td>250</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.365</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>265</td>\n",
       "      <td>46.5</td>\n",
       "      <td>1.159</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.294</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1992</td>\n",
       "      <td>6</td>\n",
       "      <td>134</td>\n",
       "      <td>70</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.542</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>60</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>43.5</td>\n",
       "      <td>0.678</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.370</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>1996</td>\n",
       "      <td>8</td>\n",
       "      <td>179</td>\n",
       "      <td>72</td>\n",
       "      <td>42</td>\n",
       "      <td>130</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.719</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>76</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.547</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1211 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  \\\n",
       "0              0            2      138             62             35        0   \n",
       "1              1            0       84             82             31      125   \n",
       "2              3            0      135             68             42      250   \n",
       "3              5            0      173             78             32      265   \n",
       "4              6            4       99             72             17        0   \n",
       "...          ...          ...      ...            ...            ...      ...   \n",
       "1206        1992            6      134             70             23      130   \n",
       "1207        1994            1       79             60             42       48   \n",
       "1208        1995            2       75             64             24       55   \n",
       "1209        1996            8      179             72             42      130   \n",
       "1210        1999            2       81             72             15       76   \n",
       "\n",
       "       BMI  DiabetesPedigreeFunction  Age  Outcome  \n",
       "0     33.6                     0.127   47        1  \n",
       "1     38.2                     0.233   23        0  \n",
       "2     42.3                     0.365   24        1  \n",
       "3     46.5                     1.159   58        0  \n",
       "4     25.6                     0.294   28        0  \n",
       "...    ...                       ...  ...      ...  \n",
       "1206  35.4                     0.542   29        1  \n",
       "1207  43.5                     0.678   23        0  \n",
       "1208  29.7                     0.370   33        0  \n",
       "1209  32.7                     0.719   36        1  \n",
       "1210  30.1                     0.547   25        0  \n",
       "\n",
       "[1211 rows x 10 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"diabetes_cleaned.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data.Outcome\n",
    "X = data.drop(['Outcome'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>138</td>\n",
       "      <td>62</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.127</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>82</td>\n",
       "      <td>31</td>\n",
       "      <td>125</td>\n",
       "      <td>38.2</td>\n",
       "      <td>0.233</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>68</td>\n",
       "      <td>42</td>\n",
       "      <td>250</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.365</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>265</td>\n",
       "      <td>46.5</td>\n",
       "      <td>1.159</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.294</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>1992</td>\n",
       "      <td>6</td>\n",
       "      <td>134</td>\n",
       "      <td>70</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.542</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>1994</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>60</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>43.5</td>\n",
       "      <td>0.678</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1995</td>\n",
       "      <td>2</td>\n",
       "      <td>75</td>\n",
       "      <td>64</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0.370</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>1996</td>\n",
       "      <td>8</td>\n",
       "      <td>179</td>\n",
       "      <td>72</td>\n",
       "      <td>42</td>\n",
       "      <td>130</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.719</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>1999</td>\n",
       "      <td>2</td>\n",
       "      <td>81</td>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>76</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.547</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1211 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  \\\n",
       "0              0            2      138             62             35        0   \n",
       "1              1            0       84             82             31      125   \n",
       "2              3            0      135             68             42      250   \n",
       "3              5            0      173             78             32      265   \n",
       "4              6            4       99             72             17        0   \n",
       "...          ...          ...      ...            ...            ...      ...   \n",
       "1206        1992            6      134             70             23      130   \n",
       "1207        1994            1       79             60             42       48   \n",
       "1208        1995            2       75             64             24       55   \n",
       "1209        1996            8      179             72             42      130   \n",
       "1210        1999            2       81             72             15       76   \n",
       "\n",
       "       BMI  DiabetesPedigreeFunction  Age  \n",
       "0     33.6                     0.127   47  \n",
       "1     38.2                     0.233   23  \n",
       "2     42.3                     0.365   24  \n",
       "3     46.5                     1.159   58  \n",
       "4     25.6                     0.294   28  \n",
       "...    ...                       ...  ...  \n",
       "1206  35.4                     0.542   29  \n",
       "1207  43.5                     0.678   23  \n",
       "1208  29.7                     0.370   33  \n",
       "1209  32.7                     0.719   36  \n",
       "1210  30.1                     0.547   25  \n",
       "\n",
       "[1211 rows x 9 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temptest, Y_train, Y_temptest = train_test_split(X, Y, test_size=0.4, shuffle = False)\n",
    "X_test_dataset, X_valid_dataset, Y_test_dataset, Y_valid_dataset =  train_test_split(X_temptest, Y_temptest, test_size=0.5, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 27.4492 - accuracy: 0.4366\n",
      "Epoch 2/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 5.3779 - accuracy: 0.5937\n",
      "Epoch 3/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.7834 - accuracy: 0.6116\n",
      "Epoch 4/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.2073 - accuracy: 0.6694\n",
      "Epoch 5/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 1.0603 - accuracy: 0.6901\n",
      "Epoch 6/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.9787 - accuracy: 0.6970\n",
      "Epoch 7/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.9044 - accuracy: 0.7080\n",
      "Epoch 8/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8862 - accuracy: 0.7039\n",
      "Epoch 9/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8461 - accuracy: 0.7039\n",
      "Epoch 10/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8068 - accuracy: 0.7094\n",
      "Epoch 11/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7928 - accuracy: 0.7163\n",
      "Epoch 12/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8718 - accuracy: 0.7121\n",
      "Epoch 13/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8241 - accuracy: 0.7190\n",
      "Epoch 14/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8583 - accuracy: 0.6928\n",
      "Epoch 15/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7202 - accuracy: 0.7231\n",
      "Epoch 16/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7602 - accuracy: 0.7259\n",
      "Epoch 17/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7782 - accuracy: 0.7204\n",
      "Epoch 18/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7224 - accuracy: 0.7383\n",
      "Epoch 19/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7833 - accuracy: 0.7039\n",
      "Epoch 20/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7615 - accuracy: 0.7149\n",
      "Epoch 21/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7147 - accuracy: 0.7218\n",
      "Epoch 22/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7193 - accuracy: 0.7121\n",
      "Epoch 23/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7135 - accuracy: 0.7452\n",
      "Epoch 24/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6724 - accuracy: 0.7176\n",
      "Epoch 25/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6446 - accuracy: 0.7273\n",
      "Epoch 26/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6739 - accuracy: 0.7314\n",
      "Epoch 27/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6192 - accuracy: 0.7369\n",
      "Epoch 28/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6643 - accuracy: 0.7135\n",
      "Epoch 29/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6446 - accuracy: 0.7094\n",
      "Epoch 30/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6300 - accuracy: 0.7383\n",
      "Epoch 31/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6816 - accuracy: 0.7218\n",
      "Epoch 32/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7517 - accuracy: 0.6983\n",
      "Epoch 33/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6292 - accuracy: 0.7369\n",
      "Epoch 34/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6755 - accuracy: 0.6942\n",
      "Epoch 35/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6537 - accuracy: 0.7369\n",
      "Epoch 36/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5985 - accuracy: 0.7259\n",
      "Epoch 37/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6007 - accuracy: 0.7424\n",
      "Epoch 38/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6313 - accuracy: 0.7355\n",
      "Epoch 39/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6088 - accuracy: 0.7355\n",
      "Epoch 40/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5831 - accuracy: 0.7342\n",
      "Epoch 41/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.7342\n",
      "Epoch 42/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5570 - accuracy: 0.7342\n",
      "Epoch 43/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5856 - accuracy: 0.7369\n",
      "Epoch 44/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5826 - accuracy: 0.7355\n",
      "Epoch 45/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6108 - accuracy: 0.7355\n",
      "Epoch 46/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.7452\n",
      "Epoch 47/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5506 - accuracy: 0.7314\n",
      "Epoch 48/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6292 - accuracy: 0.7259\n",
      "Epoch 49/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5760 - accuracy: 0.7493\n",
      "Epoch 50/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6796 - accuracy: 0.7066\n",
      "Epoch 51/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5915 - accuracy: 0.7342\n",
      "Epoch 52/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5443 - accuracy: 0.7287\n",
      "Epoch 53/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7383\n",
      "Epoch 54/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7507\n",
      "Epoch 55/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5364 - accuracy: 0.7631\n",
      "Epoch 56/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5314 - accuracy: 0.7576\n",
      "Epoch 57/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5397 - accuracy: 0.7493\n",
      "Epoch 58/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5306 - accuracy: 0.7576\n",
      "Epoch 59/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5476 - accuracy: 0.7397\n",
      "Epoch 60/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5250 - accuracy: 0.7603\n",
      "Epoch 61/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5561 - accuracy: 0.7534\n",
      "Epoch 62/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5821 - accuracy: 0.7383\n",
      "Epoch 63/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5185 - accuracy: 0.7548\n",
      "Epoch 64/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5356 - accuracy: 0.7493\n",
      "Epoch 65/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5180 - accuracy: 0.7438\n",
      "Epoch 66/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5277 - accuracy: 0.7493\n",
      "Epoch 67/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5596 - accuracy: 0.7493\n",
      "Epoch 68/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7521\n",
      "Epoch 69/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5921 - accuracy: 0.7507\n",
      "Epoch 70/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5113 - accuracy: 0.7686\n",
      "Epoch 71/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5102 - accuracy: 0.7603\n",
      "Epoch 72/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7727\n",
      "Epoch 73/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5309 - accuracy: 0.7452\n",
      "Epoch 74/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5036 - accuracy: 0.7521\n",
      "Epoch 75/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 0.7410\n",
      "Epoch 76/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.7355\n",
      "Epoch 77/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5140 - accuracy: 0.7617\n",
      "Epoch 78/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5185 - accuracy: 0.7562\n",
      "Epoch 79/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5946 - accuracy: 0.7424\n",
      "Epoch 80/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7603\n",
      "Epoch 81/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5646 - accuracy: 0.7493\n",
      "Epoch 82/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7755\n",
      "Epoch 83/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7507\n",
      "Epoch 84/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5301 - accuracy: 0.7452\n",
      "Epoch 85/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5033 - accuracy: 0.7590\n",
      "Epoch 86/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5295 - accuracy: 0.7562\n",
      "Epoch 87/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5378 - accuracy: 0.7410\n",
      "Epoch 88/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4854 - accuracy: 0.7727\n",
      "Epoch 89/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5045 - accuracy: 0.7645\n",
      "Epoch 90/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7590\n",
      "Epoch 91/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7617\n",
      "Epoch 92/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7755\n",
      "Epoch 93/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5745 - accuracy: 0.7424\n",
      "Epoch 94/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.7466\n",
      "Epoch 95/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 0.7493\n",
      "Epoch 96/150\n",
      "73/73 [==============================] - 0s 980us/step - loss: 0.5388 - accuracy: 0.7507\n",
      "Epoch 97/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5019 - accuracy: 0.7700\n",
      "Epoch 98/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.7645\n",
      "Epoch 99/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4823 - accuracy: 0.7590\n",
      "Epoch 100/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4996 - accuracy: 0.7686\n",
      "Epoch 101/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7383\n",
      "Epoch 102/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5664 - accuracy: 0.7466\n",
      "Epoch 103/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6078 - accuracy: 0.7314\n",
      "Epoch 104/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.7658\n",
      "Epoch 105/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5226 - accuracy: 0.7548\n",
      "Epoch 106/150\n",
      "73/73 [==============================] - 0s 987us/step - loss: 0.5869 - accuracy: 0.7438\n",
      "Epoch 107/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5557 - accuracy: 0.7383\n",
      "Epoch 108/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6527 - accuracy: 0.7273\n",
      "Epoch 109/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5055 - accuracy: 0.7590\n",
      "Epoch 110/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7631\n",
      "Epoch 111/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7796\n",
      "Epoch 112/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7700\n",
      "Epoch 113/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.7466\n",
      "Epoch 114/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7562\n",
      "Epoch 115/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7741\n",
      "Epoch 116/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7493\n",
      "Epoch 117/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4866 - accuracy: 0.7713\n",
      "Epoch 118/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5038 - accuracy: 0.7562\n",
      "Epoch 119/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4961 - accuracy: 0.7645\n",
      "Epoch 120/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5604 - accuracy: 0.7521\n",
      "Epoch 121/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5020 - accuracy: 0.7686\n",
      "Epoch 122/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5235 - accuracy: 0.7713\n",
      "Epoch 123/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5591 - accuracy: 0.7328\n",
      "Epoch 124/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5451 - accuracy: 0.7590\n",
      "Epoch 125/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.7741\n",
      "Epoch 126/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4843 - accuracy: 0.7645\n",
      "Epoch 127/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7893\n",
      "Epoch 128/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4688 - accuracy: 0.7727\n",
      "Epoch 129/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7810\n",
      "Epoch 130/150\n",
      "73/73 [==============================] - 0s 953us/step - loss: 0.5166 - accuracy: 0.7521\n",
      "Epoch 131/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7658\n",
      "Epoch 132/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7755\n",
      "Epoch 133/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7700\n",
      "Epoch 134/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5191 - accuracy: 0.7603\n",
      "Epoch 135/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.7617\n",
      "Epoch 136/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4839 - accuracy: 0.7713\n",
      "Epoch 137/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5095 - accuracy: 0.7727\n",
      "Epoch 138/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4788 - accuracy: 0.7769\n",
      "Epoch 139/150\n",
      "73/73 [==============================] - 0s 946us/step - loss: 0.5158 - accuracy: 0.7727\n",
      "Epoch 140/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4707 - accuracy: 0.7713\n",
      "Epoch 141/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5634 - accuracy: 0.7466\n",
      "Epoch 142/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.7948\n",
      "Epoch 143/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4792 - accuracy: 0.7837\n",
      "Epoch 144/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6130 - accuracy: 0.7452\n",
      "Epoch 145/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.7851\n",
      "Epoch 146/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.7893\n",
      "Epoch 147/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5133 - accuracy: 0.7438\n",
      "Epoch 148/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5459 - accuracy: 0.7645\n",
      "Epoch 149/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7796\n",
      "Epoch 150/150\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4975 - accuracy: 0.7686\n",
      "23/23 [==============================] - 0s 928us/step - loss: 0.4550 - accuracy: 0.7796\n",
      "Accuracy: 77.96\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape = (X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit the keras model on the dataset\n",
    "model.fit(X_train, Y_train, epochs=150, batch_size=10)\n",
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(X_train, Y_train)\n",
    "print('Accuracy: %.2f' % (accuracy*100))\n",
    "                     # Output layer => output dimension = 1 since it is a regression problem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of this model: \n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 12)                120       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 233\n",
      "Trainable params: 233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model3 = build_model3_five_hidden_layers()\n",
    "print('Here is a summary of this model: ')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=150\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "46/72 [==================>...........] - ETA: 0s - loss: 0.4924 - accuracy: 0.7913\n",
      "Epoch: 0, accuracy:0.7778,  loss:0.4847,  val_accuracy:0.7366,  val_loss:1.0909,  \n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.4847 - accuracy: 0.7778 - val_loss: 1.0909 - val_accuracy: 0.7366\n",
      "Epoch 2/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4653 - accuracy: 0.7835 - val_loss: 1.7080 - val_accuracy: 0.7284\n",
      "Epoch 3/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7654 - val_loss: 2.1151 - val_accuracy: 0.7243\n",
      "Epoch 4/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5950 - accuracy: 0.7346 - val_loss: 2.7257 - val_accuracy: 0.7243\n",
      "Epoch 5/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5170 - accuracy: 0.7528 - val_loss: 1.1362 - val_accuracy: 0.7366\n",
      "Epoch 6/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4738 - accuracy: 0.7654 - val_loss: 1.9803 - val_accuracy: 0.7243\n",
      "Epoch 7/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7696 - val_loss: 0.7752 - val_accuracy: 0.7737\n",
      "Epoch 8/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.7528 - val_loss: 0.7362 - val_accuracy: 0.7695\n",
      "Epoch 9/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7793 - val_loss: 0.5417 - val_accuracy: 0.7819\n",
      "Epoch 10/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.7709 - val_loss: 0.8262 - val_accuracy: 0.7613\n",
      "Epoch 11/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7598 - val_loss: 1.7603 - val_accuracy: 0.7243\n",
      "Epoch 12/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7654 - val_loss: 1.1367 - val_accuracy: 0.7325\n",
      "Epoch 13/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4565 - accuracy: 0.7891 - val_loss: 1.2369 - val_accuracy: 0.7325\n",
      "Epoch 14/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4484 - accuracy: 0.7961 - val_loss: 1.0464 - val_accuracy: 0.7407\n",
      "Epoch 15/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4526 - accuracy: 0.7737 - val_loss: 1.4166 - val_accuracy: 0.7325\n",
      "Epoch 16/150\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4924 - accuracy: 0.7793 - val_loss: 2.3995 - val_accuracy: 0.7243\n",
      "Epoch 17/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7598 - val_loss: 1.6850 - val_accuracy: 0.7284\n",
      "Epoch 18/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7919 - val_loss: 0.9360 - val_accuracy: 0.7531\n",
      "Epoch 19/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.7933 - val_loss: 2.1273 - val_accuracy: 0.7243\n",
      "Epoch 20/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7863 - val_loss: 1.2199 - val_accuracy: 0.7325\n",
      "Epoch 21/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.7821 - val_loss: 2.5225 - val_accuracy: 0.7243\n",
      "Epoch 22/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7416 - val_loss: 1.1944 - val_accuracy: 0.7325\n",
      "Epoch 23/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.8059 - val_loss: 0.9505 - val_accuracy: 0.7572\n",
      "Epoch 24/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5487 - accuracy: 0.7430 - val_loss: 1.8889 - val_accuracy: 0.7243\n",
      "Epoch 25/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7709 - val_loss: 0.9279 - val_accuracy: 0.7407\n",
      "Epoch 26/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.7835 - val_loss: 0.7620 - val_accuracy: 0.7654\n",
      "Epoch 27/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5129 - accuracy: 0.7640 - val_loss: 1.1091 - val_accuracy: 0.7366\n",
      "Epoch 28/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7933 - val_loss: 0.9699 - val_accuracy: 0.7366\n",
      "Epoch 29/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.7905 - val_loss: 2.3056 - val_accuracy: 0.7243\n",
      "Epoch 30/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7709 - val_loss: 0.6512 - val_accuracy: 0.7695\n",
      "Epoch 31/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.7835 - val_loss: 0.7987 - val_accuracy: 0.7613\n",
      "Epoch 32/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7682 - val_loss: 0.9734 - val_accuracy: 0.7490\n",
      "Epoch 33/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.7793 - val_loss: 0.9567 - val_accuracy: 0.7407\n",
      "Epoch 34/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.7835 - val_loss: 0.9649 - val_accuracy: 0.7407\n",
      "Epoch 35/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4455 - accuracy: 0.7961 - val_loss: 0.4556 - val_accuracy: 0.8107\n",
      "Epoch 36/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7891 - val_loss: 0.6699 - val_accuracy: 0.7695\n",
      "Epoch 37/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.7989 - val_loss: 1.9366 - val_accuracy: 0.7243\n",
      "Epoch 38/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7807 - val_loss: 1.6100 - val_accuracy: 0.7243\n",
      "Epoch 39/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4945 - accuracy: 0.7598 - val_loss: 1.2220 - val_accuracy: 0.7325\n",
      "Epoch 40/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.7947 - val_loss: 1.7074 - val_accuracy: 0.7243\n",
      "Epoch 41/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7751 - val_loss: 0.8869 - val_accuracy: 0.7407\n",
      "Epoch 42/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4340 - accuracy: 0.7835 - val_loss: 1.5612 - val_accuracy: 0.7325\n",
      "Epoch 43/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7416 - val_loss: 0.8641 - val_accuracy: 0.7654\n",
      "Epoch 44/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7668 - val_loss: 1.8917 - val_accuracy: 0.7243\n",
      "Epoch 45/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4511 - accuracy: 0.7891 - val_loss: 1.4663 - val_accuracy: 0.7284\n",
      "Epoch 46/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4599 - accuracy: 0.7919 - val_loss: 1.8544 - val_accuracy: 0.7243\n",
      "Epoch 47/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4583 - accuracy: 0.7821 - val_loss: 0.4759 - val_accuracy: 0.8189\n",
      "Epoch 48/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7849 - val_loss: 1.2337 - val_accuracy: 0.7325\n",
      "Epoch 49/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7668 - val_loss: 1.7201 - val_accuracy: 0.7284\n",
      "Epoch 50/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8003 - val_loss: 2.0389 - val_accuracy: 0.7243\n",
      "Epoch 51/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.7696 - val_loss: 1.3057 - val_accuracy: 0.7325\n",
      "Epoch 52/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7751 - val_loss: 1.8821 - val_accuracy: 0.7243\n",
      "Epoch 53/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7723 - val_loss: 2.0846 - val_accuracy: 0.7243\n",
      "Epoch 54/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4634 - accuracy: 0.7821 - val_loss: 0.9736 - val_accuracy: 0.7490\n",
      "Epoch 55/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.7933 - val_loss: 1.4911 - val_accuracy: 0.7284\n",
      "Epoch 56/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8031 - val_loss: 1.3210 - val_accuracy: 0.7284\n",
      "Epoch 57/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4719 - accuracy: 0.7682 - val_loss: 0.4418 - val_accuracy: 0.8230\n",
      "Epoch 58/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.7989 - val_loss: 0.8459 - val_accuracy: 0.7613\n",
      "Epoch 59/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.7793 - val_loss: 0.9303 - val_accuracy: 0.7490\n",
      "Epoch 60/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7891 - val_loss: 0.5729 - val_accuracy: 0.7901\n",
      "Epoch 61/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.7668 - val_loss: 1.1928 - val_accuracy: 0.7325\n",
      "Epoch 62/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.8059 - val_loss: 0.5280 - val_accuracy: 0.7984\n",
      "Epoch 63/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7989 - val_loss: 1.0065 - val_accuracy: 0.7366\n",
      "Epoch 64/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.7905 - val_loss: 1.1652 - val_accuracy: 0.7325\n",
      "Epoch 65/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5849 - accuracy: 0.7444 - val_loss: 1.7949 - val_accuracy: 0.7284\n",
      "Epoch 66/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7779 - val_loss: 1.8848 - val_accuracy: 0.7243\n",
      "Epoch 67/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7458 - val_loss: 1.2568 - val_accuracy: 0.7325\n",
      "Epoch 68/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.7682 - val_loss: 1.0866 - val_accuracy: 0.7407\n",
      "Epoch 69/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7709 - val_loss: 1.2463 - val_accuracy: 0.7325\n",
      "Epoch 70/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7696 - val_loss: 1.3019 - val_accuracy: 0.7325\n",
      "Epoch 71/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7584 - val_loss: 2.2895 - val_accuracy: 0.7243\n",
      "Epoch 72/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.7919 - val_loss: 0.5777 - val_accuracy: 0.7901\n",
      "Epoch 73/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.7975 - val_loss: 1.5295 - val_accuracy: 0.7284\n",
      "Epoch 74/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.7806 - val_loss: 1.5339 - val_accuracy: 0.7284\n",
      "Epoch 75/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.7793 - val_loss: 2.1389 - val_accuracy: 0.7243\n",
      "Epoch 76/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4588 - accuracy: 0.7737 - val_loss: 1.1103 - val_accuracy: 0.7366\n",
      "Epoch 77/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7919 - val_loss: 1.2540 - val_accuracy: 0.7407\n",
      "Epoch 78/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7500 - val_loss: 0.4602 - val_accuracy: 0.8189\n",
      "Epoch 79/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.7919 - val_loss: 0.7115 - val_accuracy: 0.7737\n",
      "Epoch 80/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.7933 - val_loss: 1.1577 - val_accuracy: 0.7325\n",
      "Epoch 81/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8045 - val_loss: 1.1074 - val_accuracy: 0.7325\n",
      "Epoch 82/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4287 - accuracy: 0.8031 - val_loss: 0.8893 - val_accuracy: 0.7490\n",
      "Epoch 83/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4176 - accuracy: 0.8212 - val_loss: 1.0135 - val_accuracy: 0.7449\n",
      "Epoch 84/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.7584 - val_loss: 2.3886 - val_accuracy: 0.7243\n",
      "Epoch 85/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4421 - accuracy: 0.8087 - val_loss: 0.5480 - val_accuracy: 0.7984\n",
      "Epoch 86/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.7668 - val_loss: 0.9351 - val_accuracy: 0.7407\n",
      "Epoch 87/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8282 - val_loss: 1.0435 - val_accuracy: 0.7407\n",
      "Epoch 88/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4549 - accuracy: 0.7821 - val_loss: 1.8617 - val_accuracy: 0.7243\n",
      "Epoch 89/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.7835 - val_loss: 0.7376 - val_accuracy: 0.7737\n",
      "Epoch 90/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.7933 - val_loss: 2.4041 - val_accuracy: 0.7243\n",
      "Epoch 91/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.8031 - val_loss: 0.7758 - val_accuracy: 0.7654\n",
      "Epoch 92/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.7933 - val_loss: 1.4608 - val_accuracy: 0.7325\n",
      "Epoch 93/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4704 - accuracy: 0.8003 - val_loss: 1.6638 - val_accuracy: 0.7243\n",
      "Epoch 94/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4932 - accuracy: 0.7793 - val_loss: 0.9870 - val_accuracy: 0.7531\n",
      "Epoch 95/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.7877 - val_loss: 0.4651 - val_accuracy: 0.8066\n",
      "Epoch 96/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4629 - accuracy: 0.7821 - val_loss: 0.8394 - val_accuracy: 0.7572\n",
      "Epoch 97/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7835 - val_loss: 0.8781 - val_accuracy: 0.7449\n",
      "Epoch 98/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.7891 - val_loss: 0.7362 - val_accuracy: 0.7737\n",
      "Epoch 99/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7682 - val_loss: 1.2481 - val_accuracy: 0.7325\n",
      "Epoch 100/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8101 - val_loss: 1.8383 - val_accuracy: 0.7243\n",
      "Epoch 101/150\n",
      "47/72 [==================>...........] - ETA: 0s - loss: 0.5188 - accuracy: 0.7639\n",
      "Epoch: 100, accuracy:0.7654,  loss:0.4949,  val_accuracy:0.7243,  val_loss:2.0790,  \n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.7654 - val_loss: 2.0790 - val_accuracy: 0.7243\n",
      "Epoch 102/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4466 - accuracy: 0.7961 - val_loss: 0.4116 - val_accuracy: 0.8354\n",
      "Epoch 103/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.7723 - val_loss: 1.2606 - val_accuracy: 0.7325\n",
      "Epoch 104/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4313 - accuracy: 0.8045 - val_loss: 0.8574 - val_accuracy: 0.7531\n",
      "Epoch 105/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4724 - accuracy: 0.7863 - val_loss: 0.4284 - val_accuracy: 0.8066\n",
      "Epoch 106/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5393 - accuracy: 0.7584 - val_loss: 1.1331 - val_accuracy: 0.7366\n",
      "Epoch 107/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.7933 - val_loss: 1.3399 - val_accuracy: 0.7325\n",
      "Epoch 108/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.7779 - val_loss: 1.2147 - val_accuracy: 0.7325\n",
      "Epoch 109/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.7779 - val_loss: 0.7071 - val_accuracy: 0.7613\n",
      "Epoch 110/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4267 - accuracy: 0.8031 - val_loss: 0.7617 - val_accuracy: 0.7613\n",
      "Epoch 111/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.7947 - val_loss: 1.2328 - val_accuracy: 0.7325\n",
      "Epoch 112/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4327 - accuracy: 0.8128 - val_loss: 1.3660 - val_accuracy: 0.7325\n",
      "Epoch 113/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.7919 - val_loss: 1.2765 - val_accuracy: 0.7325\n",
      "Epoch 114/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5412 - accuracy: 0.7318 - val_loss: 1.6445 - val_accuracy: 0.7284\n",
      "Epoch 115/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.7947 - val_loss: 0.5242 - val_accuracy: 0.8025\n",
      "Epoch 116/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4225 - accuracy: 0.8003 - val_loss: 1.2952 - val_accuracy: 0.7325\n",
      "Epoch 117/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7556 - val_loss: 0.7715 - val_accuracy: 0.7695\n",
      "Epoch 118/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.7905 - val_loss: 0.9825 - val_accuracy: 0.7449\n",
      "Epoch 119/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8115 - val_loss: 1.2550 - val_accuracy: 0.7325\n",
      "Epoch 120/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7793 - val_loss: 0.8097 - val_accuracy: 0.7654\n",
      "Epoch 121/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4218 - accuracy: 0.8017 - val_loss: 1.1565 - val_accuracy: 0.7366\n",
      "Epoch 122/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.7919 - val_loss: 1.1897 - val_accuracy: 0.7325\n",
      "Epoch 123/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4399 - accuracy: 0.7905 - val_loss: 0.4545 - val_accuracy: 0.8230\n",
      "Epoch 124/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.7919 - val_loss: 0.8922 - val_accuracy: 0.7490\n",
      "Epoch 125/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.7933 - val_loss: 1.0463 - val_accuracy: 0.7407\n",
      "Epoch 126/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.8087 - val_loss: 1.3129 - val_accuracy: 0.7325\n",
      "Epoch 127/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.8087 - val_loss: 0.7138 - val_accuracy: 0.7737\n",
      "Epoch 128/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.7947 - val_loss: 1.9572 - val_accuracy: 0.7243\n",
      "Epoch 129/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4189 - accuracy: 0.8059 - val_loss: 1.3413 - val_accuracy: 0.7366\n",
      "Epoch 130/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8101 - val_loss: 0.6246 - val_accuracy: 0.7737\n",
      "Epoch 131/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7751 - val_loss: 1.0976 - val_accuracy: 0.7407\n",
      "Epoch 132/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4300 - accuracy: 0.7919 - val_loss: 0.4942 - val_accuracy: 0.8107\n",
      "Epoch 133/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8003 - val_loss: 0.9254 - val_accuracy: 0.7407\n",
      "Epoch 134/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8156 - val_loss: 1.3902 - val_accuracy: 0.7325\n",
      "Epoch 135/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.8059 - val_loss: 1.7745 - val_accuracy: 0.7243\n",
      "Epoch 136/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4475 - accuracy: 0.8059 - val_loss: 0.6254 - val_accuracy: 0.7778\n",
      "Epoch 137/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.7835 - val_loss: 0.5904 - val_accuracy: 0.7860\n",
      "Epoch 138/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4264 - accuracy: 0.8059 - val_loss: 1.1434 - val_accuracy: 0.7325\n",
      "Epoch 139/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.7793 - val_loss: 1.5404 - val_accuracy: 0.7284\n",
      "Epoch 140/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.8073 - val_loss: 1.5035 - val_accuracy: 0.7284\n",
      "Epoch 141/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8087 - val_loss: 0.6682 - val_accuracy: 0.7778\n",
      "Epoch 142/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.7849 - val_loss: 0.7755 - val_accuracy: 0.7613\n",
      "Epoch 143/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.7975 - val_loss: 0.9651 - val_accuracy: 0.7407\n",
      "Epoch 144/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4227 - accuracy: 0.7933 - val_loss: 1.0422 - val_accuracy: 0.7407\n",
      "Epoch 145/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.7891 - val_loss: 1.3674 - val_accuracy: 0.7325\n",
      "Epoch 146/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7640 - val_loss: 1.3644 - val_accuracy: 0.7325\n",
      "Epoch 147/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.7653 - val_loss: 1.4988 - val_accuracy: 0.7284\n",
      "Epoch 148/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.7961 - val_loss: 1.5247 - val_accuracy: 0.7284\n",
      "Epoch 149/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.7849 - val_loss: 1.2736 - val_accuracy: 0.7325\n",
      "Epoch 150/150\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.4285 - accuracy: 0.8073 - val_loss: 1.7370 - val_accuracy: 0.7243\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'): # it can be with '/CPU:0'\n",
    "# with tf.device('/GPU:0'): # comment the previous line and uncomment this line to train with a GPU, if available.\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        Y_train,\n",
    "        batch_size,\n",
    "        epochs, \n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "        steps_per_epoch = int(X_train.shape[0] / batch_size) ,\n",
    "        validation_data = ([X_valid_dataset], Y_valid_dataset),\n",
    "        callbacks=[tfdocs.modeling.EpochDots(), \n",
    "                 \n",
    "                  ],\n",
    "        #\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of the results after each epoch: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.534597</td>\n",
       "      <td>0.763967</td>\n",
       "      <td>1.364354</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.484666</td>\n",
       "      <td>0.765278</td>\n",
       "      <td>1.498841</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.428483</td>\n",
       "      <td>0.796089</td>\n",
       "      <td>1.524654</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.447164</td>\n",
       "      <td>0.784916</td>\n",
       "      <td>1.273559</td>\n",
       "      <td>0.732510</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.428522</td>\n",
       "      <td>0.807263</td>\n",
       "      <td>1.736965</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy  epoch\n",
       "145  0.534597  0.763967  1.364354      0.732510    145\n",
       "146  0.484666  0.765278  1.498841      0.728395    146\n",
       "147  0.428483  0.796089  1.524654      0.728395    147\n",
       "148  0.447164  0.784916  1.273559      0.732510    148\n",
       "149  0.428522  0.807263  1.736965      0.724280    149"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Summary of the results after each epoch: ')\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/neural_without_drop_cleaned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 844us/step\n"
     ]
    }
   ],
   "source": [
    "train_predictions = model.predict(X_train).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 858us/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test_dataset).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7024793388429752\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "test_pred = np.around(test_predictions)\n",
    "\n",
    "#print(type(Y_test_dataset))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(pd.DataFrame(test_pred), Y_test_dataset))\n",
    "#print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.figure(1)\n",
    "plt.plot(test_predictions)\n",
    "plt.plot(Y_test_dataset)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(2)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize history for accuracy\n",
    "# plt.plot(history.history['mae'])\n",
    "# plt.plot(history.history['mse'])\n",
    "\n",
    "# plt.legend(['train', 'test'], loc='upper right')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.figure(3)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
